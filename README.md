# LlamaScholar ğŸ¦™ğŸ“š

**LlamaScholar** is an end-to-end research assistant that couples a fine-tuned **Llama-3 8B** model with a modern retrieval pipeline (DuckDuckGo, arXiv, and PDF-to-Chroma RAG) and serves real-time answers via **FastAPI** + SSE.  
It runs anywhere Docker runs and persists chat history in **Redis Stack**.

---

## âœ¨ Features

| Layer | What it gives you |
|-------|-------------------|
| **LLM** | Cloudflare Workers-AI wrapper by default, or your own HF model (`@hf/â€¦`). |
| **Fine-tuning** | LoRA/QLoRA scripts (`scripts/finetune_lora.py`, `merge_lora.py`) for instruction tuning on custom datasets. |
| **RAG** | `ingest_pdf.py` â†’ splits PDFs, stores chunks in **Chroma**; queried via `vector_qa` LangChain tool. |
| **Agent** | Zero-Shot-ReAct built with **LangGraph**; tools: DuckDuckGo, arXiv, vector QA. |
| **Memory** | **RedisSaver** (Redis Stack) checkpoint â€“ chat survives restarts & scales horizontally. |
| **API** | `/ask` streams tokens (Server-Sent Events), `/health` for probes, Swagger docs `/docs`. |
| **Web UI** | `static/index.html` â€“ 1-page client that streams answers live. |
| **Dev & Deploy** | Dockerfile, `docker-compose.yml`, `.env.example`; CI stub in `.github/workflows/`. |

---

## ğŸš€ Quick start (local)

```bash
git clone https://github.com/<you>/llamascholar.git
cd llamascholar
cp .env.example .env          # fill CF_ACCOUNT_ID / CF_API_TOKEN / REDIS_URL
docker compose up --build     # brings up API + Redis-Stack
# âœ http://localhost:8000/docs  â€“ test POST /ask
```
#ğŸ› ï¸ Fine-tune the model (optional)
```bash
# 1ï¸âƒ£  TSV â†’ JSONL  (instruction \t answer)
poetry run python scripts/prepare_dataset.py data/pairs.tsv data/train.jsonl

# 2ï¸âƒ£  LoRA fine-tune on single GPU (24 GB VRAM is enough)
poetry run python scripts/finetune_lora.py     # outputs lora-out/

# 3ï¸âƒ£  (Optional) merge LoRA â†’ full weights & push to HF
poetry run python scripts/merge_lora.py \
    meta-llama/Meta-Llama-3-8B-Instruct lora-out merged-out
huggingface-cli upload merged-out/* \
    --repo yourname/llamascholar-8b-finetuned
Then set in .env:

LLM_MODEL=@hf/yourname/llamascholar-8b-finetuned
```

ğŸ”Œ Environment variables (.env)

Key	Example	Required
CF_ACCOUNT_ID	abcd1234â€¦	âœ… for default Cloudflare model
CF_API_TOKEN	cf-live-tokenâ€¦	âœ…
REDIS_URL	redis://localhost:6379/0/llamascholar-chat	âœ… (use Redis Stack)
HF_HUB_TOKEN	hf_â€¦	only for pushing merged weights
ğŸ“¦ Project structure

llamascholar/
  api.py                FastAPI server
  graph_runner.py       LangGraph agent builder
  memory.py             Redis / in-memory saver
  rag_tool.py           vector_qa LangChain tool
  tool_registry.py      DuckDuckGo & arXiv tools
scripts/
  prepare_dataset.py    TSV â†’ JSONL
  finetune_lora.py      LoRA fine-tuning
  merge_lora.py         Merge LoRA into base weights
static/
  index.html            minimal streaming client

ğŸ¤ Contributing

Pull requests are welcomeâ€”please lint with ruff and run the minimal
pytest suite (make test) before opening a PR.

ğŸªª License

MIT â€” see LICENSE.
